{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YqImbTvcyWjx"
   },
   "source": [
    "## 논의하고 싶은 점\n",
    "1. 코드가 돌아가는 것 같긴 한데.. GPU가 항상 터짐. CPU로 돌리면 느릴 뿐더러 컴퓨터에서 굉음이 남..\n",
    "2. length가 512보다 크면 처리할 수 없음 -> 그냥 제외? 문단을 잘라서?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cK8sIVKyWj9"
   },
   "source": [
    "코드 출처: https://github.com/donghyeonk/movie-sentiment-analysis-kobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Aet1BRLyWkB",
    "outputId": "448aef19-be22-414f-f4ea-1058043f7278"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule  # https://pypi.org/project/pytorch-transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCmieJ3kyWkT"
   },
   "source": [
    "### KoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-po5j7SyWkX",
    "outputId": "9ff1fab0-5023-40f0-bd7d-d91f0e626d7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# load pretrained KoBERT\n",
    "model, vocab = get_pytorch_kobert_model(ctx='cpu')#'cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUbQlXF8yWkj",
    "outputId": "8b75191e-1990-4988-d443-beada319619c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(size=8002, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qxU57KpyWkv",
    "outputId": "6234fe0b-f5dd-4a09-8775-998cf6d86f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# get sentencepiece tokenizer\n",
    "tok_path = get_tokenizer()\n",
    "sp = SentencepieceTokenizer(tok_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5ZHxy-kyWk7"
   },
   "source": [
    "### torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gddxc5zPyWk-"
   },
   "outputs": [],
   "source": [
    "class MBTIDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fn_RmiZXyWlK"
   },
   "outputs": [],
   "source": [
    "def get_data(filepath, vocab, sp, max_seq_len):\n",
    "    data = list()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for lidx, l in enumerate(f):\n",
    "            if 0 == lidx:  # header는 제외\n",
    "                continue\n",
    "            cols = l[:-1].split('\\t') # 마지막 \\n 제외하고 \\t를 기준으로 분리하기\n",
    "            doc = cols[0]\n",
    "            label = cols[1]\n",
    "\n",
    "            if len(doc) > max_seq_len - 2:  # [CLS]와 [SEP] 빼고\n",
    "                doc = doc[0:(max_seq_len - 2)]\n",
    "            \n",
    "            token_ids = list()\n",
    "            token_ids.append(vocab['[CLS]'])  # 문장의 시작\n",
    "            for t in sp(doc):\n",
    "                if t in vocab: # vocab에 있는 단어라면 해당 단어의 index 찾아서 넣어주기\n",
    "                    token_ids.append(vocab[t])\n",
    "                else:          # vocab에 없다면 UNK로 처리\n",
    "                    token_ids.append(vocab['[UNK]'])\n",
    "            token_ids.append(vocab['[SEP]']) # 문장의 끝\n",
    "\n",
    "            data.append([token_ids, int(label)])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEVq-c08yWlV"
   },
   "outputs": [],
   "source": [
    "def batchify(b):\n",
    "    x_len = [len(e[0]) for e in b] # 각 doc의 길이\n",
    "    batch_max_len = max(x_len) # 제일 긴 doc의 길이\n",
    "\n",
    "    x = list()\n",
    "    tk_type_ids = list()\n",
    "    x_mask = list()\n",
    "    y = list()\n",
    "    for e in b:\n",
    "        seq_len = len(e[0])\n",
    "        e0_mask = [1] * seq_len  # 1: MASK \n",
    "        while len(e[0]) < batch_max_len:  # 제일 긴 doc의 길이에 맞추어 [PAD]로 채워주기\n",
    "            e[0].append(0)  # 0: '[PAD]'\n",
    "            e0_mask.append(0)\n",
    "        assert len(e[0]) == batch_max_len\n",
    "\n",
    "        e0_tk_type_ids = [0] * batch_max_len\n",
    "        # e0_tk_type_ids[seq_len - 1] = 1\n",
    "\n",
    "        x.append(e[0])\n",
    "        tk_type_ids.append(e0_tk_type_ids)\n",
    "        x_mask.append(e0_mask)\n",
    "        y.append(e[1])\n",
    "\n",
    "    x = torch.tensor(x, dtype=torch.int64)\n",
    "    tk_type_ids = torch.tensor(tk_type_ids, dtype=torch.int64)\n",
    "    x_mask = torch.tensor(x_mask, dtype=torch.int64)\n",
    "    y = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    return x, tk_type_ids, x_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWJ5vBTtyWlh",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = get_data('train_1.txt', vocab, sp, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKcJ9qxCyWls",
    "outputId": "25df3c5c-51a0-4827-d086-65a51576b7b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22511"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSRa2D3LyWl3",
    "outputId": "c70c432b-9281-4d7c-fd55-a607a5b073e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n"
     ]
    }
   ],
   "source": [
    "# torch setting\n",
    "lr = 5e-5\n",
    "batch_size = 16\n",
    "epochs = 5\n",
    "dropout_rate = 0.1\n",
    "max_grad_norm = 1.0\n",
    "num_total_steps = math.ceil(len(data) / batch_size) * epochs\n",
    "num_warmup_steps = num_total_steps // 10\n",
    "log_interval = 100\n",
    "seed = 2019\n",
    "num_workers = 0 #2  # 자꾸 에러 나서 일단 0으로 설정해놨음\n",
    "num_classes = 2\n",
    "pooler_out_dim = model.pooler.dense.out_features\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cpu\")#\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ddik7r9byWmD"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    MBTIDataset(data[:10]),  # 일단 10개만으로 시도\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=batchify,\n",
    "    pin_memory=True  # GPU에 필요한 argument인듯\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRR2cPGyyWmP"
   },
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(pooler_out_dim, num_classes).to(device)  # last layer for classification\n",
    "\n",
    "all_params = list(model.parameters()) + list(linear.parameters())\n",
    "optimizer = AdamW(all_params, lr=lr, correct_bias=False)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps,\n",
    "                                 t_total=num_total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwYnGOxnyWma"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, device, model, linear, all_params, optimizer, scheduler,\n",
    "      dropout_rate, max_grad_norm, log_interval, n_epochs=20, save_file_name, max_epochs_stop=3, print_every=1):\n",
    "\n",
    "    # Early stopping intialization\n",
    "    epochs_no_improve = 0\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    valid_max_acc = 0\n",
    "    history = []\n",
    "\n",
    "    # Number of epochs already trained (if using loaded in model weights)\n",
    "    try:\n",
    "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
    "    except:\n",
    "        model.epochs = 0\n",
    "        print(f'Starting Training from Scratch.\\n')\n",
    "\n",
    "    overall_start = timer()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        linear.train()\n",
    "        start = timer()\n",
    "\n",
    "        for batch_idx, (input_ids, token_type_ids, input_mask, target) in enumerate(train_loader):\n",
    "            if cuda.is_available():\n",
    "                input_ids = input_ids.to(device)\n",
    "                token_type_ids = token_type_ids.to(device)\n",
    "                input_mask = input_mask.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
    "            logits = linear(F.dropout(pooled_output, p=dropout_rate))\n",
    "            output = F.log_softmax(logits, dim=1)\n",
    "\n",
    "            loss = F.nll_loss(output, target)  # negative log-likelihood\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(all_params, max_grad_norm) # exploding gradient 방지\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "            _, pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct_tensor = pred.eq(target.view_as(pred)).sum().item()\n",
    "            #correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "\n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "        # Track training progress\n",
    "        print(f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. \n",
    "              {timer() - start:.2f} seconds elapsed in epoch.',end='\\r')\n",
    "\n",
    "      # After training loops ends, start validation      \n",
    "      else:\n",
    "        model.epochs += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "          model.eval()\n",
    "\n",
    "          for data, target in valid_loader:\n",
    "            if train_on_gpu:\n",
    "              input_ids = input_ids.to(device)\n",
    "              token_type_ids = token_type_ids.to(device)\n",
    "              input_mask = input_mask.to(device)\n",
    "              target = target.to(device)\n",
    "\n",
    "            _, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
    "            logits = linear(F.dropout(pooled_output, p=dropout_rate))\n",
    "            output = F.log_softmax(logits, dim=1)\n",
    "\n",
    "            # Validation loss\n",
    "            loss = F.nll_loss(output, target)  # negative log-likelihood\n",
    "\n",
    "            # Multiply average loss times the number of examples in batch\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "            _, pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct_tensor = pred.eq(target.view_as(pred)).sum().item()\n",
    "            #correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "\n",
    "            # Need to convert correct tensor from int to float to average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "\n",
    "            # Multiply average accuracy times the number of examples in batch\n",
    "            valid_acc += accuracy.item() * data.size(0)            \n",
    "\n",
    "        # Calculate average losses\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "\n",
    "        # Calculate average accuracy\n",
    "        train_acc = train_acc / len(train_loader.dataset)\n",
    "        valid_acc = valid_acc / len(valid_loader.dataset)\n",
    "\n",
    "        history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "\n",
    "        if (epoch + 1) % print_every == 0:\n",
    "          print('\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}')\n",
    "          print(f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%')\n",
    "\n",
    "        if valid_loss < valid_loss_min:\n",
    "          # Save model\n",
    "          torch.save(model.state_dict(), save_file_name)\n",
    "          # Track improvement\n",
    "          epochs_no_improve = 0\n",
    "          valid_loss_min = valid_loss\n",
    "          valid_best_acc = valid_acc\n",
    "          best_epoch = epoch\n",
    "\n",
    "        # Otherwise increment count of epochs with no improvement\n",
    "        else:\n",
    "          epochs_no_improve += 1\n",
    "          # Trigger early stopping\n",
    "          if epochs_no_improve >= max_epochs_stop:\n",
    "            print(f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%')\n",
    "            total_time = timer() - overall_start\n",
    "            print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.')\n",
    "\n",
    "            # Load the best state dict\n",
    "            model.load_state_dict(torch.load(save_file_name))\n",
    "            # Attach the optimizer\n",
    "            model.optimizer = optimizer\n",
    "            # Format history\n",
    "            history = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_acc','valid_acc'])\n",
    "            return model, history\n",
    "\n",
    "    # Attach the optimizer\n",
    "    model.optimizer = optimizer\n",
    "    # Record overall time and print out stats\n",
    "    total_time = timer() - overall_start\n",
    "    print(f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%')\n",
    "    print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.')\n",
    "\n",
    "    # Format history\n",
    "    history = pd.DataFrame(history,columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "\n",
    "    return model, history  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kobert_torch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
